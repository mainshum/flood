{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Incident Data Analysis\n",
    "\n",
    "This notebook documents the source data used for analyzing flood incidents across UK water utility companies. Each section below contains information about the data sources and structure for each company.\n",
    "\n",
    "## Companies Analyzed\n",
    "\n",
    "The following water utility companies were included in this analysis:\n",
    "- Anglian Water\n",
    "- Northumbrian Water\n",
    "- Penon Water\n",
    "- Severn Trent\n",
    "- Southern Water\n",
    "- United Utilities\n",
    "- Wessex Water Services Ltd\n",
    "- Yorkshire Water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anglian Water\n",
    "\n",
    "Based on this inspection, I can see that Anglian Water has three main data files with slightly different structures:\n",
    "\n",
    "**\"Flooding data 2010 to 2020.xlsx\":**\n",
    "- Sheet: \"Data Request\"\n",
    "- Contains: dates, cause codes, flooding types, locations\n",
    "- Has a legend sheet for decoding\n",
    "\n",
    "**\"2023 data.xlsx\":**\n",
    "- Sheet: \"Sheet1\"\n",
    "- Contains: dates, categories, flooding sub-types, locations\n",
    "- Includes postcodes (first half only)\n",
    "\n",
    "**\"2nd request data (1).xlsx\":**\n",
    "- Sheet: \"Data\"\n",
    "- Similar structure to 2010-2020 data\n",
    "- Has a legend sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Northumbrian Water\n",
    "\n",
    "Here's the summary for Northumbrian Water:\n",
    "\n",
    "**1. EIR22807 Sewer flooding incident data 2010 to 2023.xlsx**\n",
    "- Sheet: Sheet1\n",
    "- Columns:\n",
    "  - DATE\n",
    "  - LOCATION\n",
    "  - Cause\n",
    "  - Year\n",
    "  - Weather\n",
    "  - Postcode\n",
    "  - NOTE: (contains explanatory text)\n",
    "\n",
    "**2. EIR22807 Clean water flooding, incidents by area.xlsx**\n",
    "- Sheets: 2020, 2019\n",
    "- Columns:\n",
    "  - Postal Area\n",
    "  - Incident Count\n",
    "- Note: This appears to be aggregated data by area, not individual incidents\n",
    "\n",
    "**3. EIR22727 Sewer flooding incident data 2021 2022 2023.xlsx**\n",
    "- Sheet: Sewer flooding incidents\n",
    "- Note: This appears to be aggregated summary data with years as columns, not individual incidents\n",
    "\n",
    "**4. EIR22727 Clean water flooding incident data 2021 2022 2023.xlsx**\n",
    "- Sheet: Clean water flooding incidents\n",
    "- Columns:\n",
    "  - Postal Area\n",
    "  - Incident Count\n",
    "- Note: This appears to be aggregated data by area, not individual incidents\n",
    "\n",
    "Based on this analysis, only the first file (EIR22807 Sewer flooding incident data 2010 to 2023.xlsx) contains granular incident-level data. The other files contain aggregated summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penon Water\n",
    "\n",
    "**Note:** Source data information for Penon Water is not available in the documentation. The company was included in the analysis but the specific data structure and source files are not documented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severn Trent\n",
    "\n",
    "**1. Flooding incidents EIR2025-046.xlsx**\n",
    "- Sheet: Sewer Water Incident Data\n",
    "- Columns:\n",
    "  - WW Job Reference\n",
    "  - Date Reported\n",
    "  - Job Type\n",
    "  - High Level Fault\n",
    "  - Sewer Conduit\n",
    "  - Sewer Status\n",
    "  - Depth of Flooding (m)\n",
    "  - Postcode\n",
    "  - Responsibilty\n",
    "  - Weather\n",
    "\n",
    "**2. Sewer Flooding Incident Data 2023 EIR2024 079.xlsx**\n",
    "- Sheet: Sewer flooding incident data 23\n",
    "- Columns:\n",
    "  - Rapid Reference\n",
    "  - Date Reported\n",
    "  - Job Type\n",
    "  - High Level Fault\n",
    "  - Sewer Conduit\n",
    "  - Sewer Status\n",
    "  - Sewer Type\n",
    "  - Postcode\n",
    "\n",
    "**3. 21-23 data/2021 2023 Flooding incidents EIR2024 131.xlsx**\n",
    "- Sheet: Sewer Water Incident Data\n",
    "- Columns:\n",
    "  - WW Job Reference\n",
    "  - Date Reported\n",
    "  - Job Type\n",
    "  - High Level Fault\n",
    "  - Sewer Conduit\n",
    "  - Sewer Status\n",
    "  - Depth of Flooding (m)\n",
    "  - Postcode\n",
    "  - Compensation (£)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Southern Water\n",
    "\n",
    "Now I have a good understanding of Southern Water's data structure. They have:\n",
    "\n",
    "**Main Southern Water data: 2023 Sewer Incidents.xlsx** with sheets:\n",
    "- \"Sewer Incidents 2023\" - main data\n",
    "- \"suspicious (louis)\" - additional data\n",
    "- \"Summary\" - aggregated data (skip)\n",
    "\n",
    "**Southwest Water data:**\n",
    "- EIR24187.xlsx - 2023 data\n",
    "- 2nd request/1405 Flooding data.xlsx - historical data with legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## United Utilities\n",
    "\n",
    "Here's the summary of United Utilities' data structure:\n",
    "\n",
    "**1. EIR 2023 Flooding.xlsx**\n",
    "- Sheet: Flooding_2023\n",
    "- Columns:\n",
    "  - INCIDENT YEAR\n",
    "  - INCIDENT RESPONSIBILITY\n",
    "  - INCIDENT DATE\n",
    "  - CATEGORY\n",
    "  - NUMBER OF IMPACTED PROPERTIES\n",
    "  - INCIDENT CAUSE\n",
    "  - POSTCODE\n",
    "\n",
    "**2. EIR-380 - Flooding Incidents Data.xlsb**\n",
    "- Sheets: Internal, External\n",
    "- Columns:\n",
    "  - Incident Date (Excel serial, needs conversion)\n",
    "  - Impacted Customer Postcode\n",
    "  - Flooding Type\n",
    "  - Flooding Location\n",
    "  - Flooding Cause\n",
    "  - Flooding Depth\n",
    "\n",
    "**3. 2nd request/EIR 260 Flooding Incidents Data.xlsx**\n",
    "- Sheets: FY21, FY22, FY23\n",
    "- Columns:\n",
    "  - Date\n",
    "  - Part Postcode\n",
    "  - Water Type\n",
    "  - Compensation Claimed Y/N\n",
    "  - Compensation Amount\n",
    "  - Incident Type\n",
    "  - Cause\n",
    "  - Depth of Flood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wessex Water Services Ltd:\n",
    "\n",
    "### 1. Flooding incidents EIR2025-046.xlsx\n",
    "- Sheet: `Sewer Water Incident Data`\n",
    "- Columns:\n",
    "  - WW Job Reference\n",
    "  - Date Reported\n",
    "  - Job Type\n",
    "  - High Level Fault\n",
    "  - Sewer Conduit\n",
    "  - Sewer Status\n",
    "  - Depth of Flooding (m)\n",
    "  - Postcode\n",
    "  - Responsibilty\n",
    "  - Weather\n",
    "\n",
    "### 2. Sewer Flooding Incident Data 2023 EIR2024 079.xlsx\n",
    "- Sheet: `Sewer flooding incident data 23`\n",
    "- Columns:\n",
    "  - Rapid Reference\n",
    "  - Date Reported\n",
    "  - Job Type\n",
    "  - High Level Fault\n",
    "  - Sewer Conduit\n",
    "  - Sewer Status\n",
    "  - Sewer Type\n",
    "  - Postcode\n",
    "\n",
    "### 3. 21-23 data/2021 2023 Flooding incidents EIR2024 131.xlsx\n",
    "- Sheet: `Sewer Water Incident Data`\n",
    "- Columns:\n",
    "  - WW Job Reference\n",
    "  - Date Reported\n",
    "  - Job Type\n",
    "  - High Level Fault\n",
    "  - Sewer Conduit\n",
    "  - Sewer Status\n",
    "  - Depth of Flooding (m)\n",
    "  - Postcode\n",
    "  - Compensation (£)\n",
    "\n",
    "All three files have similar structures with granular incident data. I'll extract:\n",
    "- Date of incident: `Date Reported`\n",
    "- Incident type: combine `Job Type`, `High Level Fault`\n",
    "- Location: `Postcode`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yorkshire Water\n",
    "\n",
    "Here's the summary for Yorkshire Water:\n",
    "\n",
    "**EIR 937.xlsx (Sheet1)**\n",
    "- Columns:\n",
    "  - Town\n",
    "  - Postcode Prefix\n",
    "  - Responsibility\n",
    "  - Flooding source\n",
    "  - Int/Ext\n",
    "  - Curtilage/Non-Curtilage\n",
    "  - GSS Value\n",
    "  - Inc date\n",
    "  - Depth of flood\n",
    "\n",
    "**EIR 996.xlsx (EIR 966 Final)**\n",
    "- Columns:\n",
    "  - Town\n",
    "  - Postcode Prefix\n",
    "  - Responsibility\n",
    "  - Flooding Source\n",
    "  - Int/Ext/RTU\n",
    "  - Curtilage/Non Curtilage\n",
    "  - Inc Date\n",
    "  - Depth of flood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penon Water\n",
    "\n",
    "Based on the processing code analysis, Penon Water data was sourced from two main files:\n",
    "\n",
    "**1. EIR25077.xlsx**\n",
    "- Sheets: \n",
    "  - \"External Sewer Floodings2010-23\" - external flooding incidents\n",
    "  - \"Internal Sewer Floodings2010-23\" - internal flooding incidents\n",
    "- Columns:\n",
    "  - Post Code\n",
    "  - Raised Date\n",
    "  - Flooding Cause (with standardized legend mapping)\n",
    "  - Location of Flooding\n",
    "- Cause codes were mapped to standardized descriptions:\n",
    "  - BLPR: Blockage paper rag\n",
    "  - BLFT: Blockage fat\n",
    "  - BLST: Blockage silt\n",
    "  - BLDB: Blockage non sewage debris\n",
    "  - BLRT: Blockage roots\n",
    "  - CLBU: Collapse/burst\n",
    "  - PACB/PTCB: Partial collapse\n",
    "  - EQFL: Equipment failure\n",
    "  - HYOL: Hydraulic overload\n",
    "  - HOPS: Hydraulically overloaded pumping station\n",
    "  - SEWC: Sewer condition\n",
    "  - TPDM: Third party damage\n",
    "  - PSBL: Pump station blockage\n",
    "  - PSBR: Pump station breakdown\n",
    "\n",
    "**2. EIR24187.xlsx**\n",
    "- Sheet: \"Data\"\n",
    "- Columns:\n",
    "  - Date Raised\n",
    "  - Town/City\n",
    "  - Postcode\n",
    "  - Flooding Category\n",
    "  - Feedback Responsibility\n",
    "  - Feedback Cause\n",
    "- Flooding categories were converted to lowercase for standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Notes\n",
    "\n",
    "Based on the examination, the Ofwat data appears to be aggregated summary data rather than individual incident records. The sheets contain:\n",
    "- External sewer flooding: Company-level aggregated data with years as columns\n",
    "- Internal sewer flooding: Similar aggregated structure with company names and yearly statistics\n",
    "\n",
    "Since the focus was on granular incident-level data and to skip aggregated information, the Ofwat data was skipped as it contains summary statistics rather than individual flood incidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--output OUTPUT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/aprzeszlo/Library/Jupyter/runtime/kernel-v33eb7e93581b9b1cdaeac10cc0c7489bd75a8e748.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aprzeszlo/repos/floods/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3680: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "def concatenate_parquet_files():\n",
    "    \"\"\"Reuse the concatenation logic from preview_results.py\"\"\"\n",
    "    results_path = Path('results')\n",
    "    parquet_files = list(results_path.glob('*.parquet'))\n",
    "    \n",
    "    if not parquet_files:\n",
    "        print(\"No parquet files found in results folder\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"Found {len(parquet_files)} parquet files in results folder:\")\n",
    "    for file in parquet_files:\n",
    "        print(f\"  - {file.name}\")\n",
    "    print()\n",
    "    \n",
    "    # Read and concatenate all parquet files\n",
    "    dfs = []\n",
    "    for file in parquet_files:\n",
    "        df_temp = pd.read_parquet(file)\n",
    "        dfs.append(df_temp)\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Concatenated {len(df)} total rows from {len(parquet_files)} files\")\n",
    "    print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "Here's the summary for\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "def concatenate_parquet_files():\n",
    "    \"\"\"Reuse the concatenation logic from preview_results.py\"\"\"\n",
    "    results_path = Path('results')\n",
    "    parquet_files = list(results_path.glob('*.parquet'))\n",
    "    \n",
    "    if not parquet_files:\n",
    "        print(\"No parquet files found in results folder\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"Found {len(parquet_files)} parquet files in results folder:\")\n",
    "    for file in parquet_files:\n",
    "        print(f\"  - {file.name}\")\n",
    "    print()\n",
    "    \n",
    "    # Read and concatenate all parquet files\n",
    "    dfs = []\n",
    "    for file in parquet_files:\n",
    "        df_temp = pd.read_parquet(file)\n",
    "        dfs.append(df_temp)\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Concatenated {len(df)} total rows from {len(parquet_files)} files\")\n",
    "    print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Generate incident type counts table from all parquet files in results folder')\n",
    "parser.add_argument('--output', '-o', help='Output file path (optional - if not provided, prints to console)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "try:\n",
    "    # Concatenate all parquet files\n",
    "    df = concatenate_parquet_files()\n",
    "    \n",
    "    # Check if incident_type column exists\n",
    "    if 'incident_type' not in df.columns:\n",
    "        print(\"Error: 'incident_type' column not found in the data\", file=sys.stderr)\n",
    "        print(\"Available columns:\", df.columns.tolist(), file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Generate incident type counts\n",
    "    incident_counts = df['incident_type'].value_counts()\n",
    "    \n",
    "    # Create a formatted table\n",
    "    print(\"=== INCIDENT TYPE COUNTS ===\")\n",
    "    print(f\"Total incidents: {len(df)}\")\n",
    "    print(f\"Unique incident types: {len(incident_counts)}\")\n",
    "    print()\n",
    "    \n",
    "    # Create a DataFrame for better formatting\n",
    "    counts_df = pd.DataFrame({\n",
    "        'Incident Type': incident_counts.index,\n",
    "        'Count': incident_counts.values,\n",
    "        'Percentage': (incident_counts.values / len(df) * 100).round(2)\n",
    "    })\n",
    "    \n",
    "    # Display the table\n",
    "    print(counts_df.to_string(index=False))\n",
    "    \n",
    "    # Save to file if output path provided\n",
    "    if args.output:\n",
    "        counts_df.to_csv(args.output, index=False)\n",
    "        print(f\"\\nResults saved to: {args.output}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\", file=sys.stderr)\n",
    "    sys.exit(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
